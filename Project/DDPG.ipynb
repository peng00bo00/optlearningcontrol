{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pengbo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Action Space:  Box(1,)\n",
      "Action Space H :  [1.]\n",
      "Obsrev Space:  Box(11,)\n",
      "Obsrev H:  [inf inf inf inf inf inf inf inf inf inf inf]\n",
      "Obsrev L:  [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "os.system('clear')\n",
    "\n",
    "\n",
    "env_name = 'InvertedDoublePendulum-v2'\n",
    "env = gym.make(env_name)\n",
    "print('############################################')\n",
    "print('Action Space: ', env.action_space)\n",
    "print('Action Space H : ', env.action_space.high)\n",
    "\n",
    "print('Obsrev Space: ', env.observation_space)\n",
    "print('Obsrev H: ',env.observation_space.high)\n",
    "print('Obsrev L: ',env.observation_space.low)\n",
    "print('############################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS    = 11\n",
    "NUM_ACTION    = 1\n",
    "\n",
    "GAMMA         = 0.99\n",
    "CRITIC_ALPHA  = 1e-3\n",
    "ACTOR_ALPHA   = 1e-4\n",
    "ACTION_BOUNDS = np.array([env.action_space.high],dtype=np.float32)\n",
    "NUM_EPISODES  = 1000\n",
    "NUM_EPOCHS = 5000\n",
    "MEMORY_SIZE   = 1e5\n",
    "TAU           = 0.001\n",
    "MINIBATCH_SIZE= 128\n",
    "RENDER_SKIP   = 20\n",
    "OrnUhl_SIGMA  = 0.05\n",
    "OrnUhl_MEAN   = np.zeros((NUM_ACTION,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "            len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.clipped_gradients = [tf.clip_by_norm(grad , 5.0) for grad in self.unnormalized_actor_gradients]\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.clipped_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tf.placeholder(tf.float32,shape=(None,self.s_dim))\n",
    "        net = tf.layers.dense(inputs,400)\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.layers.dense(net,300)\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "        net = tf.nn.relu(net)\n",
    "        w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "        out =  tf.layers.dense(net,self.a_dim,activation = tf.nn.tanh,kernel_initializer = w_init )\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tf.losses.mean_squared_error(self.predicted_q_value, self.out)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tf.placeholder(tf.float32,shape=(None,self.s_dim))\n",
    "        action = tf.placeholder(tf.float32,shape=(None,self.a_dim))\n",
    "        net = tf.layers.dense(inputs,400)\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "        net = tf.nn.relu(net)\n",
    "        # Add the action tensor in the 2nd hidden layer\n",
    "        # Use two temp layers to get the corresponding weights and biases\n",
    "        t1 = tf.layers.dense(net,300)\n",
    "        t2 = tf.layers.dense(action,300)\n",
    "\n",
    "        net = tf.add(t1, t2)\n",
    "        net = tf.layers.batch_normalization(net)\n",
    "        # net = tf.matmul(net, t1.kernel) + tf.matmul(action, t2w.kernel) + t2.bias\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "        out =  tf.layers.dense(net,1,kernel_initializer = w_init )\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self,max_size):\n",
    "        self.internal_mem = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def sample_batch(self,batch_size):\n",
    "        rnd_smpl_mem = random.sample(self.internal_mem,min(len(self.internal_mem),batch_size))\n",
    "        temp_state0 = []\n",
    "        temp_action = []\n",
    "        temp_reward = []\n",
    "        temp_state = []\n",
    "        temp_done = []\n",
    "        for mem in rnd_smpl_mem:\n",
    "            temp_state0.append(mem[0])\n",
    "            temp_action.append(mem[1])\n",
    "            temp_reward.append(mem[2])\n",
    "            temp_state.append(mem[3])\n",
    "            temp_done.append(mem[4])\n",
    "        return temp_state0, temp_action, temp_reward, temp_state, temp_done\n",
    "\n",
    "        # return list(zip(*temp_mem))[0],list(zip(*temp_mem))[1],list(zip(*temp_mem))[2],list(zip(*temp_mem))[3],list(zip(*temp_mem))[4]\n",
    "\n",
    "    def remember(self,new_data):\n",
    "        self.internal_mem.append(new_data)\n",
    "        if len(self.internal_mem) > self.max_size:\n",
    "            self.internal_mem.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNoise(object):\n",
    "    def reset(self):\n",
    "        pass\n",
    "class OrnsteinUhlenbeckActionNoise(ActionNoise): # or replace 'ActionNoise' with 'object'\n",
    "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-bac3935e559c>:55: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/pengbo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-bac3935e559c>:56: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-5-bac3935e559c>:43: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "# Critic & Actor Networks\n",
    "actor = ActorNetwork(session,NUM_INPUTS,NUM_ACTION,3,CRITIC_ALPHA,TAU,MINIBATCH_SIZE)\n",
    "critic = CriticNetwork(session,NUM_INPUTS,NUM_ACTION,ACTOR_ALPHA,TAU,GAMMA,actor.get_num_trainable_vars())\n",
    "\n",
    "\n",
    "memory = Memory(MEMORY_SIZE)\n",
    "ornuhl_noise = OrnsteinUhlenbeckActionNoise(OrnUhl_MEAN,OrnUhl_SIGMA)\n",
    "\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg100_score=[]\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        state0 = env.reset()\n",
    "        episode_rwd = 0\n",
    "        t = 0\n",
    "        done = False\n",
    "        ornuhl_noise.reset()\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            if (epoch < 1) or ( (episode%9 in [0,4])&(epoch<15) ):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = actor.predict([state0])[0][0]\n",
    "\n",
    "                noise = ornuhl_noise.__call__().reshape(-1,NUM_ACTION)[0]\n",
    "                action += noise\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_rwd += reward\n",
    "            t+= 1\n",
    "\n",
    "            # Render\n",
    "            if episode%RENDER_SKIP == 0:\n",
    "                env.render()\n",
    "\n",
    "            # save to memeory and get random minibatch\n",
    "            memory.remember((state0,action,reward,state,done))\n",
    "            # if epoch > 5:\n",
    "            state0_minibatch,action_minibatch,reward_minibatch,state_minibatch,done_minibatch = memory.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "            trgt_actor_pred_minibatch = actor.predict_target(state_minibatch)\n",
    "            trgt_q_state_minibatch = critic.predict_target(state_minibatch,trgt_actor_pred_minibatch)\n",
    "\n",
    "            # Y_ = np.array(reward_minibatch + GAMMA*trgt_q_state_minibatch).reshape(-1,1)\n",
    "            # print(Y_.shape)\n",
    "            # Y_minibatch = list(([x] for x in Y_))\n",
    "\n",
    "            Y_minibatch = []\n",
    "\n",
    "            for ix in range(len(state_minibatch)):\n",
    "                if done_minibatch[ix]:\n",
    "                    Y_minibatch.append(np.array(reward_minibatch[ix], dtype=np.float32))\n",
    "                else:\n",
    "                    Y_minibatch.append(reward_minibatch[ix] + GAMMA*trgt_q_state_minibatch[ix])\n",
    "\n",
    "            Y_ = np.array(Y_minibatch).reshape(-1,1)\n",
    "\n",
    "            critic.train(state0_minibatch,action_minibatch,Y_)\n",
    "            actor_pred0_minibatch = actor.predict(state0_minibatch)\n",
    "\n",
    "            critic_gradients = critic.action_gradients(state0_minibatch,actor_pred0_minibatch)[0]\n",
    "            actor.train(state0_minibatch,critic_gradients)\n",
    "\n",
    "            critic.update_target_network()\n",
    "            actor.update_target_network()\n",
    "\n",
    "\n",
    "            state0 = state\n",
    "\n",
    "        avg100_score.append(episode_rwd)\n",
    "        avg100_score = avg100_score[-100:]\n",
    "        avg100 = np.mean(avg100_score)\n",
    "        print('Epoch {} Episode {} Reward {} Steps {} <><> Avg100 Score {}'.format(epoch,episode,episode_rwd,t,avg100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
